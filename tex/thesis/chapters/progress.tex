\section{Progress}

Our task is to do \textit{IoT data analytics with serverless computing}, therefore as a first step we started out by thinking about the underlying infrastructure. We decided on using Apache Kafka for streaming,  \textit{MongoDB} as our database and \textit{OpenFaaS} as the serverless platform. \\
The decision to use \textit{OpenFaaS} was made because of its ease to deploy a stack of services using \textit{Docker Swarm}.
While doing research we came across many different serverless frameworks. \textit{OpenWhisk}, \textit{Fission} and \textit{Kubeless} just to name a few. While all of those seem to have their benefits, none of them seemed to be as versatile as \textit{OpenFaaS}. \textit{OpenFaaS} poses itself to have first class support for \textit{Docker Swarm} and being \textit{Kubernetes} native. The former was particularly interesting for us as this meant that we could test the framework without having to install any external tool except for  \textit{Docker}. Running was as simple as cloning the \textit{OpenFaaS} repository, calling \texttt{docker swarm init} and executing the provided initialization script. Deploying an actual function is equally as simple. One has the option to either deploy a function from the store through the nicely designed  \textit{OpenFaaS} gateway on port 8080 or with the prefered way, which is using the \textit{faas-cli}. Deploying a function from the store with it would look as follows

\begin{lstlisting}[language=bash]
  $ faas store deploy figlet
\end{lstlisting}

where \texttt{figlet} is the name of the function in the store.\\
\\
After gaining a grasp of how the platform works, we decided to put our own spin on it by firstly modifying the given deploy script to our needs and porting it from \textit{Bash} to \textit{Rust} for it to be cross platform. The next step then was to write our own configuration file for swarm deployment, namely \texttt{deploy.yml}. This \textit{YAML} file includes the configuration for \textit{Kafka}, \textit{Zookeeper}, \textit{MongoDB}, various services that are needed for \textit{OpenFaaS}, a bunch of gateways for visualisation and the \textit{Kafka-Connector}. The latter is particularly interesting, because its main purpose is to call a serverless function on a Kafka topic change. To make a function react to a Kafka topic we can again use our example store function \texttt{figlet}

\begin{lstlisting}[language=bash]
  $ faas store deploy figlet --annotation topic="faas-request"
\end{lstlisting}

The deployment aspect is the same as before therefore the interesting part is the \texttt{-{}-annotation} flag, where \texttt{topic="faas-request"} is the \textit{Kafka} topic the function is supposed to listen to and act on. We subsequently can look for the result in the logs of the connector service. \\
While our technology stack for the most part was nice to work with, we still had our fair share of problems. The most significant so far was in relation to our \textit{MongoDB} database. \textit{MongoDB's} API design is confusing to say the least. Questionable deprecation choices and multiple connect interfaces are the most rampant examples. Unfortunately those were not the only problems in that regard we ran into. Connecting through a node instance natively on the system would work fine, however connecting through a deployed function in \textit{OpenFaaS} was not possible. After applying various fixes the function was finally able to connect. \\
The next challenge will be to post to the database through the \textit{Kafka-Connector}. When that is done we can finally focus on gathering data from IoT devices.
